Automatically generated by Mendeley Desktop 1.16.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@inproceedings{Zhao2011,
abstract = {Real-time unusual event detection in video stream has been a difficult challenge due to the lack of sufficient training information, volatility of the definitions for both normality and abnormality, time constraints, and statistical limitation of the fitness of any parametric models. We propose a fully unsupervised dynamic sparse coding approach for detecting unusual events in videos based on online sparse re-constructibility of query signals from an atomically learned event dictionary, which forms a sparse coding bases. Based on an intuition that usual events in a video are more likely to be reconstructible from an event dictionary, whereas unusual events are not, our algorithm employs a principled convex optimization formulation that allows both a sparse reconstruction code, and an online dictionary to be jointly inferred and updated. Our algorithm is completely un-supervised, making no prior assumptions of what unusual events may look like and the settings of the cameras. The fact that the bases dictionary is updated in an online fashion as the algorithm observes more data, avoids any issues with concept drift. Experimental results on hours of real world surveillance video and several Youtube videos show that the proposed algorithm could reliably locate the unusual events in the video sequence, outperforming the current state-of-the-art methods.},
author = {Zhao, Bin and Fei-Fei, Li and Xing, Eric P.},
booktitle = {CVPR 2011},
doi = {10.1109/CVPR.2011.5995524},
isbn = {978-1-4577-0394-2},
issn = {10636919},
month = {jun},
pages = {3313--3320},
publisher = {IEEE},
title = {{Online detection of unusual events in videos via dynamic sparse coding}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5995524},
year = {2011}
}
@article{Zubair2013,
abstract = {Audio classification is an important problem in signal processing and pattern recognition with potential applications in audio retrieval, documentation and scene analysis. Common to general signal classification systems, it involves both training and classification (or testing) stages. The performance of an audio classification system, such as its complexity and classification accuracy, depends highly on the choice of the signal features and the classifiers. Several features have been widely exploited in existing methods, such as the mel-frequency cepstrum coefficients (MFCCs), line spectral frequencies (LSF) and short time energy (STM). In this paper, instead of using these well-established features, we explore the potential of sparse features, derived from the dictionary of signal atoms using sparse coding based on e.g. orthogonal matching pursuit (OMP), where the atoms are adapted directly from audio training data using the K-SVD dictionary learning algorithm. To reduce the computational complexity, we propose to perform pooling and sampling operations on the sparse coefficients. Such operations also help to maintain a unified dimension of the signal features, regardless of the various lengths of the training and testing signals. Using the popular support vector machine (SVM) as the classifier, we examine the performance of the proposed classification system for two binary classification problems, namely speechâ€“music classification and maleâ€“female speech discrimination and a multi-class problem, speaker identification. The experimental results show that the sparse (max-pooled and average-pooled) coefficients perform better than the classical MFCCs features, in particular, for noisy audio data.},
author = {Zubair, Syed and Yan, Fei and Wang, Wenwu},
doi = {10.1016/j.dsp.2013.01.004},
journal = {Digital Signal Processing},
keywords = {Audio classification,Dictionary learning,Sparse coefficients,Support vector machines},
pages = {960--970},
title = {{Digital Signal Processing Dictionary learning based sparse coefficients for audio classification with max and average pooling}},
url = {www.elsevier.com/locate/dsp},
volume = {23},
year = {2013}
}
\@article{Mairal2012,
author = {Mairal, Julien and Bach, Francis and Ponce, Jean},
doi = {10.1561/0600000058},
journal = {Computer Graphics and Vision},
number = {3},
pages = {85--283},
title = {{Sparse Modeling for Image and Vision Processing}},
volume = {8},
year = {2012}
}
@inproceedings{Mairal2009,
abstract = {Sparse codingâ€”that is, modelling data vectors as sparse linear combinations of basis elementsâ€”is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on learning the basis set, also called dictionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the audio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic approximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dictionaries than classical batch algorithms for both small and large datasets.},
address = {New York, New York, USA},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
doi = {10.1145/1553374.1553463},
isbn = {9781605585161},
pages = {689--696},
publisher = {ACM Press},
title = {{Online dictionary learning for sparse coding}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553463},
year = {2009}
}
@article{Elad2006,
abstract = {We address the image denoising problem, where zero-mean white and homogeneous Gaussian additive noise is to be removed from a given image. The approach taken is based on sparse and redundant representations over trained dictionaries. Using the K-SVD algorithm, we obtain a dictionary that describes the image content effectively. Two training options are considered: using the corrupted image itself, or training on a corpus of high-quality image database. Since the K-SVD is limited in handling small image patches, we extend its deployment to arbitrary image sizes by defining a global image prior that forces sparsity over patches in every location in the image. We show how such Bayesian treatment leads to a simple and effective denoising algorithm. This leads to a state-of-the-art denoising performance, equivalent and sometimes surpassing recently published leading alternative denoising methods.},
author = {Elad, Michael and Aharon, Michal},
doi = {10.1109/TIP.2006.881969},
isbn = {978-1-4244-5237-8},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
month = {dec},
number = {12},
pages = {3736--3745},
pmid = {17153947},
publisher = {IEEE},
title = {{Image Denoising Via Sparse and Redundant Representations Over Learned Dictionaries}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4011956},
volume = {15},
year = {2006}
}
@article{Chen1998,
abstract = {The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries â€” stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). Basis Pursuit (BP) is a principle for decomposing a signal into an " optimal " superposition of dictionary elements, where optimal means having the smallest l 1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising. BP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.},
author = {Chen, Scott Shaobing and Donoho, David L and Saunders, Michael A},
doi = {10.1137/S1064827596304010},
issn = {1064-8275},
journal = {SIAM Journal on Scientific Computing},
keywords = {1 norm optimization,41A45 PII S1064827596304010,65D15,65K05,cosine packets,denoising,interior-point methods for linear programming,matching pursuit,multiscale edges AMS subject classifications 94A12,overcomplete signal representation,time-frequency analysis,time-scale analysis,total variation denoising,wavelet packets,wavelets},
month = {jan},
number = {1},
pages = {33--61},
title = {{Atomic Decomposition by Basis Pursuit}},
url = {http://epubs.siam.org/doi/abs/10.1137/S1064827596304010},
volume = {20},
year = {1998}
}
@inproceedings{Lee2007,
abstract = {Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that cap-ture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L 1 -regularized least squares problem and an L 2 -constrained least squares problem. We propose novel algorithms to solve both of these optimiza-tion problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field sur-round suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons.},
author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew Y},
booktitle = {Advances in Neural Information Processing Systems},
pages = {801--808},
title = {{Efficient sparse coding algorithms}},
year = {2007}
}
@misc{Santos2014,
    author = {Jo‹o Felipe Santos},
    title  = {Dictionary Learning and Sparse Coding for Speech Signals},
    month  = {feb},
    year   = {2014},
    url    = "\url{http://www.seaandsailor.com/dictlearning.html}",
    note = "[Online; retrieved on 7-August-2016 from \url{http://www.seaandsailor.com/dictlearning.html}]"
}
@misc{wiki:xxx,
   author = "Wikipedia",
   title = "Bohemian Rhapsody --- Wikipedia{,} The Free Encyclopedia",
   year = "2016",
   url = "\url{https://en.wikipedia.org/w/index.php?title=Bohemian_Rhapsody&oldid=732647147}",
   note = "[Online; retrieved on 7-August-2016 from \url{https://en.wikipedia.org/w/index.php?title=Bohemian_Rhapsody&oldid=732647147}]"
 }